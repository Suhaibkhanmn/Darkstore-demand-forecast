{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0f0f1c1-01a3-4719-b21a-ffff2c0cea63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: C:\\Users\\mnsuh\\OneDrive\\Desktop\\darkstore\\venv\\Scripts\\python.exe\n"
     ]
    }
   ],
   "source": [
    "# Two-Stage Demand Forecasting (Favorita) â€” Occurrence + Size\n",
    "\n",
    "import os, gc, json, warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ---- Paths ----\n",
    "RAW = \"data/raw\"\n",
    "PROCESSED = \"data/processed\"\n",
    "MODELS = \"models\"\n",
    "REPORTS = \"reports\"\n",
    "os.makedirs(PROCESSED, exist_ok=True)\n",
    "os.makedirs(MODELS, exist_ok=True)\n",
    "os.makedirs(REPORTS, exist_ok=True)\n",
    "\n",
    "# ---- Config ----\n",
    "HORIZON_DAYS = 14\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "SPEED_MODE = True\n",
    "DATE_START = \"2017-01-01\"\n",
    "MAX_STORES = 10\n",
    "MAX_ITEMS  = 200\n",
    "\n",
    "import sys\n",
    "print(\"Python:\", sys.executable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6ce7646-14d8-4206-ae72-5fb766ce77f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>store_id</th>\n",
       "      <th>sku_id</th>\n",
       "      <th>qty</th>\n",
       "      <th>is_promo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>3</td>\n",
       "      <td>AUTOMOTIVE</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-01-02</td>\n",
       "      <td>3</td>\n",
       "      <td>AUTOMOTIVE</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-01-03</td>\n",
       "      <td>3</td>\n",
       "      <td>AUTOMOTIVE</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-01-04</td>\n",
       "      <td>3</td>\n",
       "      <td>AUTOMOTIVE</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-01-05</td>\n",
       "      <td>3</td>\n",
       "      <td>AUTOMOTIVE</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date store_id      sku_id   qty  is_promo\n",
       "0 2017-01-01        3  AUTOMOTIVE   0.0         0\n",
       "1 2017-01-02        3  AUTOMOTIVE   9.0         0\n",
       "2 2017-01-03        3  AUTOMOTIVE   5.0         0\n",
       "3 2017-01-04        3  AUTOMOTIVE  19.0         0\n",
       "4 2017-01-05        3  AUTOMOTIVE  13.0         0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Peek at the header to choose the correct columns\n",
    "hdr = pd.read_csv(f\"{RAW}/train.csv\", nrows=3)\n",
    "\n",
    "if \"item_nbr\" in hdr.columns:\n",
    "    # Older 2017 Favorita\n",
    "    USECOLS = [\"date\", \"store_nbr\", \"item_nbr\", \"unit_sales\", \"onpromotion\"]\n",
    "    KEY_ITEM = \"item_nbr\"\n",
    "    TARGET   = \"unit_sales\"\n",
    "else:\n",
    "    # Newer \"Store Sales\" competition (most likely your case)\n",
    "    USECOLS = [\"date\", \"store_nbr\", \"family\", \"sales\", \"onpromotion\"]\n",
    "    KEY_ITEM = \"family\"\n",
    "    TARGET   = \"sales\"\n",
    "\n",
    "train = pd.read_csv(f\"{RAW}/train.csv\", usecols=USECOLS, parse_dates=[\"date\"])\n",
    "\n",
    "# Standardize to our schema\n",
    "df = train.rename(columns={\n",
    "    \"date\": \"date\",\n",
    "    \"store_nbr\": \"store_id\",\n",
    "    KEY_ITEM: \"sku_id\",\n",
    "    TARGET: \"qty\",\n",
    "    \"onpromotion\": \"is_promo\"\n",
    "})\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "df[\"qty\"] = df[\"qty\"].clip(lower=0)\n",
    "df[\"is_promo\"] = df[\"is_promo\"].fillna(False).astype(int)\n",
    "\n",
    "# Optional: trim to a smaller slice for laptop training\n",
    "if SPEED_MODE:\n",
    "    df = df[df[\"date\"] >= DATE_START].copy()\n",
    "    top_stores = (df.groupby(\"store_id\")[\"qty\"].sum()\n",
    "                    .sort_values(ascending=False).head(MAX_STORES).index)\n",
    "    df = df[df[\"store_id\"].isin(top_stores)]\n",
    "    top_items = (df.groupby(\"sku_id\")[\"qty\"].sum()\n",
    "                   .sort_values(ascending=False).head(MAX_ITEMS).index)\n",
    "    df = df[df[\"sku_id\"].isin(top_items)]\n",
    "\n",
    "# Categories save memory and help LightGBM\n",
    "for c in [\"store_id\",\"sku_id\"]:\n",
    "    df[c] = df[c].astype(\"category\")\n",
    "\n",
    "df.sort_values([\"store_id\",\"sku_id\",\"date\"], inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "458a95e2-87cc-4a44-9b15-7c00179cf3b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>store_id</th>\n",
       "      <th>sku_id</th>\n",
       "      <th>qty</th>\n",
       "      <th>is_promo</th>\n",
       "      <th>transactions</th>\n",
       "      <th>oil_price</th>\n",
       "      <th>holiday_flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>3</td>\n",
       "      <td>AUTOMOTIVE</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3088.0</td>\n",
       "      <td>53.75</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-01-02</td>\n",
       "      <td>3</td>\n",
       "      <td>AUTOMOTIVE</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3918.0</td>\n",
       "      <td>53.75</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-01-03</td>\n",
       "      <td>3</td>\n",
       "      <td>AUTOMOTIVE</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3406.0</td>\n",
       "      <td>52.36</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-01-04</td>\n",
       "      <td>3</td>\n",
       "      <td>AUTOMOTIVE</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3382.0</td>\n",
       "      <td>53.26</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-01-05</td>\n",
       "      <td>3</td>\n",
       "      <td>AUTOMOTIVE</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3109.0</td>\n",
       "      <td>53.77</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date  store_id      sku_id   qty  is_promo  transactions  oil_price  \\\n",
       "0 2017-01-01         3  AUTOMOTIVE   0.0         0        3088.0      53.75   \n",
       "1 2017-01-02         3  AUTOMOTIVE   9.0         0        3918.0      53.75   \n",
       "2 2017-01-03         3  AUTOMOTIVE   5.0         0        3406.0      52.36   \n",
       "3 2017-01-04         3  AUTOMOTIVE  19.0         0        3382.0      53.26   \n",
       "4 2017-01-05         3  AUTOMOTIVE  13.0         0        3109.0      53.77   \n",
       "\n",
       "   holiday_flag  \n",
       "0             0  \n",
       "1             0  \n",
       "2             0  \n",
       "3             0  \n",
       "4             0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load side tables\n",
    "stores = pd.read_csv(f\"{RAW}/stores.csv\")  # has store_nbr, city, state, etc.\n",
    "hol    = pd.read_csv(f\"{RAW}/holidays_events.csv\", parse_dates=[\"date\"])\n",
    "oil    = pd.read_csv(f\"{RAW}/oil.csv\", parse_dates=[\"date\"]).rename(columns={\"dcoilwtico\":\"oil_price\"})\n",
    "txn    = pd.read_csv(f\"{RAW}/transactions.csv\", parse_dates=[\"date\"])\n",
    "\n",
    "# Align transactions store column name\n",
    "txn.rename(columns={\"store_nbr\":\"store_id\"}, inplace=True)\n",
    "\n",
    "# Merge transactions\n",
    "if SPEED_MODE:\n",
    "    txn = txn[txn[\"date\"] >= df[\"date\"].min()]\n",
    "df = df.merge(txn, how=\"left\", on=[\"store_id\",\"date\"])\n",
    "df[\"transactions\"] = df.groupby(\"store_id\")[\"transactions\"].transform(lambda s: s.fillna(s.median()))\n",
    "df[\"transactions\"] = df[\"transactions\"].fillna(df[\"transactions\"].median())\n",
    "\n",
    "# Merge oil; fill gaps\n",
    "oil = oil.sort_values(\"date\")\n",
    "oil[\"oil_price\"] = oil[\"oil_price\"].ffill().bfill()\n",
    "df = df.merge(oil[[\"date\",\"oil_price\"]], how=\"left\", on=\"date\")\n",
    "df[\"oil_price\"] = df[\"oil_price\"].ffill().bfill()\n",
    "\n",
    "# Simple holiday flag: mark only real holidays (ignore Work Day / Transfer)\n",
    "hol_flag = (hol[hol[\"type\"].isin([\"Holiday\",\"Additional\"])]\n",
    "              .loc[~hol[\"transferred\"].fillna(False), [\"date\"]]\n",
    "              .drop_duplicates())\n",
    "hol_flag[\"holiday_flag\"] = 1\n",
    "df = df.merge(hol_flag, on=\"date\", how=\"left\")\n",
    "df[\"holiday_flag\"] = df[\"holiday_flag\"].fillna(0).astype(int)\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82249eb8-9a98-4427-80e5-de375b0bc37b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(        date  store_id      sku_id   qty  is_promo  transactions  oil_price  \\\n",
       " 0 2017-01-01         3  AUTOMOTIVE   0.0         0        3088.0      53.75   \n",
       " 1 2017-01-02         3  AUTOMOTIVE   9.0         0        3918.0      53.75   \n",
       " 2 2017-01-03         3  AUTOMOTIVE   5.0         0        3406.0      52.36   \n",
       " 3 2017-01-04         3  AUTOMOTIVE  19.0         0        3382.0      53.26   \n",
       " 4 2017-01-05         3  AUTOMOTIVE  13.0         0        3109.0      53.77   \n",
       " \n",
       "    holiday_flag  \n",
       " 0             0  \n",
       " 1             0  \n",
       " 2             0  \n",
       " 3             0  \n",
       " 4             0  ,\n",
       " (74910, 8))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def reindex_group(g):\n",
    "    idx = pd.date_range(g[\"date\"].min(), g[\"date\"].max(), freq=\"D\")\n",
    "    g = g.set_index(\"date\").reindex(idx)\n",
    "    g.index.name = \"date\"\n",
    "    g[\"store_id\"] = g[\"store_id\"].iloc[0]\n",
    "    g[\"sku_id\"]   = g[\"sku_id\"].iloc[0]\n",
    "    g[\"qty\"] = g[\"qty\"].fillna(0)\n",
    "    g[\"is_promo\"] = g[\"is_promo\"].fillna(0).astype(int)\n",
    "    for c in [\"transactions\",\"oil_price\",\"holiday_flag\"]:\n",
    "        if c in g.columns:\n",
    "            g[c] = g[c].ffill().bfill().fillna(0)\n",
    "    return g.reset_index()\n",
    "\n",
    "df = (df.groupby([\"store_id\",\"sku_id\"], group_keys=False)\n",
    "        .apply(reindex_group)\n",
    "        .sort_values([\"store_id\",\"sku_id\",\"date\"])\n",
    "        .reset_index(drop=True))\n",
    "df.head(), df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df7b0884-8d39-4425-b341-b96fadcc90eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((65670, 22),\n",
       "         date store_id      sku_id   qty  is_promo  transactions  oil_price  \\\n",
       " 0 2017-01-29        3  AUTOMOTIVE   8.0         0        3406.0      53.18   \n",
       " 1 2017-01-30        3  AUTOMOTIVE  15.0         0        2671.0      52.63   \n",
       " 2 2017-01-31        3  AUTOMOTIVE   4.0         0        2938.0      52.75   \n",
       " 3 2017-02-01        3  AUTOMOTIVE   6.0         0        3347.0      53.90   \n",
       " 4 2017-02-02        3  AUTOMOTIVE   7.0         0        2890.0      53.55   \n",
       " \n",
       "    holiday_flag  dow  month  ...  lag_1  lag_7  lag_14  roll7_mean  roll7_std  \\\n",
       " 0             0    6      1  ...   18.0   22.0    23.0   13.571429   6.502747   \n",
       " 1             0    0      1  ...    8.0   12.0     1.0   11.571429   5.563486   \n",
       " 2             0    1      1  ...   15.0    3.0     7.0   12.000000   5.715476   \n",
       " 3             0    2      2  ...    4.0   10.0    17.0   12.142857   5.459810   \n",
       " 4             0    3      2  ...    6.0   11.0     2.0   11.571429   5.912054   \n",
       " \n",
       "    roll28_mean  exp_decay_7  store_roll7_mean  occ  size  \n",
       " 0    11.571429    12.459923       1661.454718    1   8.0  \n",
       " 1    11.857143    12.014228       1840.519566    1  15.0  \n",
       " 2    12.071429    12.310927       1253.297857    1   4.0  \n",
       " 3    12.035714    11.489247       1225.210571    1   6.0  \n",
       " 4    11.571429    10.949019       2115.626143    1   7.0  \n",
       " \n",
       " [5 rows x 22 columns])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def add_calendar(X):\n",
    "    d = pd.to_datetime(X[\"date\"])\n",
    "    X[\"dow\"] = d.dt.weekday\n",
    "    X[\"month\"] = d.dt.month\n",
    "    X[\"is_weekend\"] = (X[\"dow\"]>=5).astype(int)\n",
    "    X[\"is_month_end\"] = d.dt.is_month_end.astype(int)\n",
    "    return X\n",
    "\n",
    "def add_lags_rolls(X, target=\"qty\"):\n",
    "    X = X.sort_values([\"store_id\",\"sku_id\",\"date\"]).copy()\n",
    "    g = X.groupby([\"store_id\",\"sku_id\"], sort=False)\n",
    "    X[\"lag_1\"]  = g[target].shift(1)\n",
    "    X[\"lag_7\"]  = g[target].shift(7)\n",
    "    X[\"lag_14\"] = g[target].shift(14)\n",
    "    X[\"roll7_mean\"]  = g[target].shift(1).rolling(7).mean()\n",
    "    X[\"roll7_std\"]   = g[target].shift(1).rolling(7).std()\n",
    "    X[\"roll28_mean\"] = g[target].shift(1).rolling(28).mean()\n",
    "    X[\"exp_decay_7\"] = g[target].shift(1).ewm(halflife=7, min_periods=3).mean()\n",
    "    return X\n",
    "\n",
    "def add_store_prior(X, target=\"qty\"):\n",
    "    X = X.sort_values([\"store_id\",\"date\"]).copy()\n",
    "    X[\"store_roll7_mean\"] = X.groupby(\"store_id\")[target].shift(1).rolling(7).mean()\n",
    "    return X\n",
    "\n",
    "feat = df.copy()\n",
    "feat = add_calendar(feat)\n",
    "feat = add_lags_rolls(feat, target=\"qty\")\n",
    "feat = add_store_prior(feat, target=\"qty\")\n",
    "\n",
    "feature_cols = [\n",
    "    \"dow\",\"month\",\"is_weekend\",\"is_month_end\",\n",
    "    \"lag_1\",\"lag_7\",\"lag_14\",\"roll7_mean\",\"roll7_std\",\"roll28_mean\",\"exp_decay_7\",\n",
    "    \"store_roll7_mean\",\n",
    "    \"is_promo\",\"transactions\",\"oil_price\",\"holiday_flag\"\n",
    "]\n",
    "\n",
    "# Clean NaNs from the early days of each series\n",
    "for c in feature_cols:\n",
    "    feat[c] = feat[c].replace([np.inf,-np.inf], np.nan)\n",
    "\n",
    "def drop_cold_start(g):\n",
    "    m = g[\"roll28_mean\"].isna()\n",
    "    if m.any():\n",
    "        first_ok = (~m).idxmax()\n",
    "        return g.loc[first_ok:]\n",
    "    return g\n",
    "\n",
    "feat = (feat.groupby([\"store_id\",\"sku_id\"], group_keys=False)\n",
    "            .apply(drop_cold_start)\n",
    "            .reset_index(drop=True))\n",
    "\n",
    "feat[feature_cols] = feat[feature_cols].fillna(0.0)\n",
    "\n",
    "# Targets for two-stage\n",
    "feat[\"occ\"]  = (feat[\"qty\"]>0).astype(int)\n",
    "feat[\"size\"] = feat[\"qty\"].clip(lower=0).astype(float)\n",
    "\n",
    "for c in [\"store_id\",\"sku_id\"]:\n",
    "    feat[c] = feat[c].astype(\"category\")\n",
    "\n",
    "feat.shape, feat.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a744971-137e-49eb-aaf7-bc5169ce1696",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_cv_indices(dates: pd.Series, n_splits=4, min_train_days=150, holdout_days=HORIZON_DAYS):\n",
    "    unique_days = np.array(sorted(pd.unique(pd.to_datetime(dates))))\n",
    "    folds = []\n",
    "    for i in range(n_splits):\n",
    "        train_end = len(unique_days) - (n_splits - i) * holdout_days\n",
    "        if train_end < min_train_days:\n",
    "            continue\n",
    "        tr_days = unique_days[:train_end]\n",
    "        va_days = unique_days[train_end:train_end+holdout_days]\n",
    "        folds.append((tr_days, va_days))\n",
    "    for tr_days, va_days in folds:\n",
    "        tr_idx = np.where(pd.to_datetime(dates).isin(tr_days))[0]\n",
    "        va_idx = np.where(pd.to_datetime(dates).isin(va_days))[0]\n",
    "        yield tr_idx, va_idx\n",
    "\n",
    "def WAPE(y_true, y_pred, eps=1e-9):\n",
    "    y_true, y_pred = np.asarray(y_true), np.asarray(y_pred)\n",
    "    return float(np.sum(np.abs(y_true - y_pred)) / (np.sum(np.abs(y_true)) + eps))\n",
    "\n",
    "def SMAPE(y_true, y_pred, eps=1e-9):\n",
    "    y_true, y_pred = np.asarray(y_true), np.asarray(y_pred)\n",
    "    return float(np.mean(2*np.abs(y_true - y_pred)/(np.abs(y_true)+np.abs(y_pred)+eps)))\n",
    "\n",
    "def BIAS(y_true, y_pred, eps=1e-9):\n",
    "    y_true, y_pred = np.asarray(y_true), np.asarray(y_pred)\n",
    "    return float(np.sum(y_pred - y_true) / (np.sum(np.abs(y_true))+eps))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a91db07-ad11-472c-abe1-0c9996dda03a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                 | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 48396, number of negative: 3414\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004372 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2580\n",
      "[LightGBM] [Info] Number of data points in the train set: 51810, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.934105 -> initscore=2.651533\n",
      "[LightGBM] [Info] Start training from score 2.651533\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003799 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2580\n",
      "[LightGBM] [Info] Number of data points in the train set: 48396, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 6.986608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                                | 1/3 [00:10<00:21, 10.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 52630, number of negative: 3800\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002585 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2591\n",
      "[LightGBM] [Info] Number of data points in the train set: 56430, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.932660 -> initscore=2.628285\n",
      "[LightGBM] [Info] Start training from score 2.628285\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002218 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2591\n",
      "[LightGBM] [Info] Number of data points in the train set: 52630, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 6.983918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                        | 2/3 [00:19<00:09,  9.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 56816, number of negative: 4234\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002748 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2606\n",
      "[LightGBM] [Info] Number of data points in the train set: 61050, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.930647 -> initscore=2.596671\n",
      "[LightGBM] [Info] Start training from score 2.596671\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002731 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2606\n",
      "[LightGBM] [Info] Number of data points in the train set: 56816, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 6.983361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:28<00:00,  9.64s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'wape': 0.12381067320174077,\n",
       " 'smape': 0.4050997645856913,\n",
       " 'bias': 0.07559840381273646,\n",
       " 'occ_auc': 0.9922994627145019}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dates = feat[\"date\"]\n",
    "X = feat[feature_cols]\n",
    "y_occ  = feat[\"occ\"]\n",
    "y_size = feat[\"size\"]\n",
    "y_true = feat[\"qty\"].values\n",
    "\n",
    "preds_oof = np.zeros(len(feat))\n",
    "fold_summ = []\n",
    "\n",
    "clf = LGBMClassifier(\n",
    "    n_estimators=600, learning_rate=0.05, num_leaves=127,\n",
    "    subsample=0.9, colsample_bytree=0.9, random_state=RANDOM_STATE\n",
    ")\n",
    "reg = LGBMRegressor(\n",
    "    objective=\"tweedie\", tweedie_variance_power=1.2,\n",
    "    n_estimators=900, learning_rate=0.045, num_leaves=255,\n",
    "    subsample=0.9, colsample_bytree=0.9, reg_alpha=1.0, reg_lambda=2.0,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "for tr_idx, va_idx in tqdm(list(rolling_cv_indices(dates, n_splits=4, min_train_days=150, holdout_days=HORIZON_DAYS))):\n",
    "    Xtr, Xva = X.iloc[tr_idx], X.iloc[va_idx]\n",
    "    ytr_occ, yva_occ = y_occ.iloc[tr_idx], y_occ.iloc[va_idx]\n",
    "    ytr_size, yva_size = y_size.iloc[tr_idx], y_size.iloc[va_idx]\n",
    "    yva_true = y_true[va_idx]\n",
    "\n",
    "    # Stage 1: occurrence\n",
    "    clf.fit(Xtr, ytr_occ)\n",
    "    p_occ = clf.predict_proba(Xva)[:,1]\n",
    "\n",
    "    # Stage 2: size (train only on positives)\n",
    "    mask_pos = ytr_size > 0\n",
    "    reg.fit(Xtr[mask_pos], ytr_size[mask_pos])\n",
    "\n",
    "    size_pred = np.clip(reg.predict(Xva), 0, None)\n",
    "    yhat = p_occ * size_pred\n",
    "\n",
    "    preds_oof[va_idx] = yhat\n",
    "    fold_wape = WAPE(yva_true, yhat)\n",
    "    fold_smape = SMAPE(yva_true, yhat)\n",
    "    fold_bias = BIAS(yva_true, yhat)\n",
    "    try:\n",
    "        fold_auc = roc_auc_score(yva_occ, p_occ)\n",
    "    except Exception:\n",
    "        fold_auc = np.nan\n",
    "\n",
    "    fold_summ.append({\"wape\":fold_wape, \"smape\":fold_smape, \"bias\":fold_bias, \"occ_auc\":fold_auc})\n",
    "    gc.collect()\n",
    "\n",
    "cv_avg = pd.DataFrame(fold_summ).mean(numeric_only=True).to_dict()\n",
    "cv_avg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f4b108f9-24d8-4cd6-805f-5b899c88af12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>wape</th>\n",
       "      <th>smape</th>\n",
       "      <th>bias</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>0.824098</td>\n",
       "      <td>1.600704</td>\n",
       "      <td>-0.788247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B</td>\n",
       "      <td>0.822670</td>\n",
       "      <td>1.598367</td>\n",
       "      <td>-0.792289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C</td>\n",
       "      <td>0.826055</td>\n",
       "      <td>1.557253</td>\n",
       "      <td>-0.780675</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  class      wape     smape      bias\n",
       "0     A  0.824098  1.600704 -0.788247\n",
       "1     B  0.822670  1.598367 -0.792289\n",
       "2     C  0.826055  1.557253 -0.780675"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A/B/C by cumulative volume within each (store, sku) pair overall\n",
    "vol = feat.groupby([\"store_id\",\"sku_id\"])[\"qty\"].sum().sort_values(ascending=False)\n",
    "cum = (vol.cumsum()/vol.sum()).reset_index(name=\"cumshare\")\n",
    "abc = pd.merge(vol.reset_index(name=\"total_qty\"), cum, on=[\"store_id\",\"sku_id\"])\n",
    "def tag(x):\n",
    "    if x <= 0.2: return \"A\"\n",
    "    if x <= 0.5: return \"B\"\n",
    "    return \"C\"\n",
    "abc[\"class\"] = abc[\"cumshare\"].apply(tag)\n",
    "\n",
    "# Map back to rows\n",
    "key_series = feat[\"store_id\"].astype(str) + \"||\" + feat[\"sku_id\"].astype(str)\n",
    "abc_key = abc.copy()\n",
    "abc_key[\"key\"] = abc_key[\"store_id\"].astype(str) + \"||\" + abc_key[\"sku_id\"].astype(str)\n",
    "class_map = dict(zip(abc_key[\"key\"], abc_key[\"class\"]))\n",
    "feat[\"abc_class\"] = key_series.map(class_map)\n",
    "\n",
    "# Metrics by class\n",
    "by_class = []\n",
    "for cls in [\"A\",\"B\",\"C\"]:\n",
    "    m = feat[\"abc_class\"]==cls\n",
    "    if m.any():\n",
    "        by_class.append({\n",
    "            \"class\": cls,\n",
    "            \"wape\": WAPE(feat.loc[m,\"qty\"], preds_oof[m]),\n",
    "            \"smape\": SMAPE(feat.loc[m,\"qty\"], preds_oof[m]),\n",
    "            \"bias\": BIAS(feat.loc[m,\"qty\"], preds_oof[m]),\n",
    "        })\n",
    "pd.DataFrame(by_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa940f9c-f9af-46c6-a860-7a6c53bfb7f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 61019, number of negative: 4651\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005161 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2616\n",
      "[LightGBM] [Info] Number of data points in the train set: 65670, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.929176 -> initscore=2.574103\n",
      "[LightGBM] [Info] Start training from score 2.574103\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004508 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2616\n",
      "[LightGBM] [Info] Number of data points in the train set: 61019, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 6.977711\n",
      "Saved:\n",
      "- reports/oof_predictions.csv\n",
      "- reports/cv_metrics.json\n",
      "- models/two_stage_lgbm.pkl\n"
     ]
    }
   ],
   "source": [
    "# Save predictions & metrics\n",
    "out = feat[[\"store_id\",\"sku_id\",\"date\",\"qty\"]].copy()\n",
    "out[\"pred_oof\"] = preds_oof\n",
    "out.to_csv(f\"{REPORTS}/oof_predictions.csv\", index=False)\n",
    "\n",
    "with open(f\"{REPORTS}/cv_metrics.json\",\"w\") as f:\n",
    "    json.dump({\"cv_avg\":cv_avg, \"by_class\":by_class}, f, indent=2)\n",
    "\n",
    "# Train final models on ALL data & save\n",
    "clf.fit(X, y_occ)\n",
    "mask_pos_all = y_size > 0\n",
    "reg.fit(X[mask_pos_all], y_size[mask_pos_all])\n",
    "\n",
    "import joblib\n",
    "joblib.dump({\"occ_model\": clf, \"size_model\": reg, \"features\": feature_cols}, f\"{MODELS}/two_stage_lgbm.pkl\")\n",
    "\n",
    "print(\"Saved:\", f\"{REPORTS}/oof_predictions.csv\", f\"{REPORTS}/cv_metrics.json\", f\"{MODELS}/two_stage_lgbm.pkl\", sep=\"\\n- \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "adf6dfe7-f546-460d-be54-c982742424ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OVERALL (pre-debias): {'wape': 0.8246967285404175, 'smape': 1.559413472356703, 'bias': -0.785505751702002}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>wape</th>\n",
       "      <th>smape</th>\n",
       "      <th>bias</th>\n",
       "      <th>rows</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>0.824098</td>\n",
       "      <td>1.600704</td>\n",
       "      <td>-0.788247</td>\n",
       "      <td>1194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B</td>\n",
       "      <td>0.822670</td>\n",
       "      <td>1.598367</td>\n",
       "      <td>-0.792289</td>\n",
       "      <td>2189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C</td>\n",
       "      <td>0.826055</td>\n",
       "      <td>1.557253</td>\n",
       "      <td>-0.780675</td>\n",
       "      <td>62287</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  class      wape     smape      bias   rows\n",
       "0     A  0.824098  1.600704 -0.788247   1194\n",
       "1     B  0.822670  1.598367 -0.792289   2189\n",
       "2     C  0.826055  1.557253 -0.780675  62287"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you still have `feat` and `preds_oof` in memory, use them.\n",
    "# Otherwise fall back to reports/oof_predictions.csv (works either way).\n",
    "\n",
    "import numpy as np, pandas as pd, json, os\n",
    "\n",
    "if \"feat\" in globals() and \"preds_oof\" in globals():\n",
    "    work = feat[[\"store_id\",\"sku_id\",\"date\",\"qty\"]].copy()\n",
    "    work[\"pred_oof\"] = preds_oof\n",
    "else:\n",
    "    work = pd.read_csv(\"reports/oof_predictions.csv\")\n",
    "    # make sure types are consistent\n",
    "    for c in [\"store_id\",\"sku_id\"]:\n",
    "        if c in work.columns:\n",
    "            work[c] = work[c].astype(str)\n",
    "\n",
    "# ---- Build A/B/C classes by cumulative volume over (store_id, sku_id) ----\n",
    "pair_vol = (work.groupby([\"store_id\",\"sku_id\"])[\"qty\"]\n",
    "                .sum()\n",
    "                .sort_values(ascending=False))\n",
    "cumshare = (pair_vol.cumsum() / pair_vol.sum())\n",
    "\n",
    "def _cls(x):\n",
    "    if x <= 0.20: return \"A\"\n",
    "    if x <= 0.50: return \"B\"\n",
    "    return \"C\"\n",
    "\n",
    "pair_class = cumshare.to_frame(\"cumshare\").assign(abc_class=cumshare.map(_cls))\n",
    "pair_class = pair_class[\"abc_class\"]\n",
    "\n",
    "# Map class back to rows\n",
    "key = work[\"store_id\"].astype(str) + \"||\" + work[\"sku_id\"].astype(str)\n",
    "pair_key = (pair_class.reset_index()\n",
    "                     .assign(key=lambda d: d[\"store_id\"].astype(str) + \"||\" + d[\"sku_id\"].astype(str))\n",
    "                     .set_index(\"key\")[\"abc_class\"])\n",
    "work[\"abc_class\"] = key.map(pair_key).fillna(\"C\")\n",
    "\n",
    "# ---- Metrics helpers ----\n",
    "def WAPE(y, yhat, eps=1e-9):\n",
    "    y = np.asarray(y); yhat = np.asarray(yhat)\n",
    "    return float(np.abs(y - yhat).sum() / (np.abs(y).sum() + eps))\n",
    "\n",
    "def SMAPE(y, yhat, eps=1e-9):\n",
    "    y = np.asarray(y); yhat = np.asarray(yhat)\n",
    "    return float(np.mean(2*np.abs(y - yhat)/(np.abs(y)+np.abs(yhat)+eps)))\n",
    "\n",
    "def BIAS(y, yhat, eps=1e-9):\n",
    "    y = np.asarray(y); yhat = np.asarray(yhat)\n",
    "    return float((yhat - y).sum() / (np.abs(y).sum() + eps))\n",
    "\n",
    "# ---- Overall + by-class metrics (pre-debias) ----\n",
    "overall = {\n",
    "    \"wape\": WAPE(work[\"qty\"], work[\"pred_oof\"]),\n",
    "    \"smape\": SMAPE(work[\"qty\"], work[\"pred_oof\"]),\n",
    "    \"bias\": BIAS(work[\"qty\"], work[\"pred_oof\"]),\n",
    "}\n",
    "by_class = []\n",
    "for cls in [\"A\",\"B\",\"C\"]:\n",
    "    m = work[\"abc_class\"]==cls\n",
    "    if m.any():\n",
    "        by_class.append({\n",
    "            \"class\": cls,\n",
    "            \"wape\": WAPE(work.loc[m,\"qty\"], work.loc[m,\"pred_oof\"]),\n",
    "            \"smape\": SMAPE(work.loc[m,\"qty\"], work.loc[m,\"pred_oof\"]),\n",
    "            \"bias\": BIAS(work.loc[m,\"qty\"], work.loc[m,\"pred_oof\"]),\n",
    "            \"rows\": int(m.sum())\n",
    "        })\n",
    "\n",
    "print(\"OVERALL (pre-debias):\", overall)\n",
    "pd.DataFrame(by_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "203fedc2-f897-49c3-a33c-570158e97ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OVERALL (post-debias): {'wape': 1.6005504626904081, 'smape': 1.7655955956964817, 'bias': -1.1376944155282864e-16, 'debias_factor': 4.662129674501549}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>wape</th>\n",
       "      <th>smape</th>\n",
       "      <th>bias</th>\n",
       "      <th>rows</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>1.590590</td>\n",
       "      <td>1.858106</td>\n",
       "      <td>-0.012782</td>\n",
       "      <td>1194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B</td>\n",
       "      <td>1.574518</td>\n",
       "      <td>1.857170</td>\n",
       "      <td>-0.031623</td>\n",
       "      <td>2189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C</td>\n",
       "      <td>1.618874</td>\n",
       "      <td>1.760604</td>\n",
       "      <td>0.022523</td>\n",
       "      <td>62287</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  class      wape     smape      bias   rows\n",
       "0     A  1.590590  1.858106 -0.012782   1194\n",
       "1     B  1.574518  1.857170 -0.031623   2189\n",
       "2     C  1.618874  1.760604  0.022523  62287"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bring average bias ~ 0 by scaling predictions so total forecast == total actual.\n",
    "den = float(work[\"pred_oof\"].sum()) + 1e-9\n",
    "num = float(work[\"qty\"].sum())\n",
    "debias_factor = num / den\n",
    "\n",
    "work[\"pred_oof_debiased\"] = work[\"pred_oof\"] * debias_factor\n",
    "\n",
    "overall_deb = {\n",
    "    \"wape\": WAPE(work[\"qty\"], work[\"pred_oof_debiased\"]),\n",
    "    \"smape\": SMAPE(work[\"qty\"], work[\"pred_oof_debiased\"]),\n",
    "    \"bias\": BIAS(work[\"qty\"], work[\"pred_oof_debiased\"]),\n",
    "    \"debias_factor\": debias_factor\n",
    "}\n",
    "by_class_deb = []\n",
    "for cls in [\"A\",\"B\",\"C\"]:\n",
    "    m = work[\"abc_class\"]==cls\n",
    "    if m.any():\n",
    "        by_class_deb.append({\n",
    "            \"class\": cls,\n",
    "            \"wape\": WAPE(work.loc[m,\"qty\"], work.loc[m,\"pred_oof_debiased\"]),\n",
    "            \"smape\": SMAPE(work.loc[m,\"qty\"], work.loc[m,\"pred_oof_debiased\"]),\n",
    "            \"bias\": BIAS(work.loc[m,\"qty\"], work.loc[m,\"pred_oof_debiased\"]),\n",
    "            \"rows\": int(m.sum())\n",
    "        })\n",
    "\n",
    "print(\"OVERALL (post-debias):\", overall_deb)\n",
    "pd.DataFrame(by_class_deb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b50dd3e-6907-4b87-89d5-1f87931f25e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved:\n",
      "- reports/oof_predictions_debiased.csv\n",
      "- reports/cv_metrics_fixed.json\n",
      "- models/debias_factor.json\n"
     ]
    }
   ],
   "source": [
    "# Write updated OOF with debiased preds\n",
    "os.makedirs(\"reports\", exist_ok=True)\n",
    "work_out = work[[\"store_id\",\"sku_id\",\"date\",\"qty\",\"pred_oof\",\"pred_oof_debiased\",\"abc_class\"]].copy()\n",
    "work_out.to_csv(\"reports/oof_predictions_debiased.csv\", index=False)\n",
    "\n",
    "# Save a tidy metrics JSON (pre/post)\n",
    "metrics_payload = {\n",
    "    \"overall_pre\": overall,\n",
    "    \"overall_post\": overall_deb,\n",
    "    \"by_class_pre\": by_class,\n",
    "    \"by_class_post\": by_class_deb\n",
    "}\n",
    "with open(\"reports/cv_metrics_fixed.json\",\"w\") as f:\n",
    "    json.dump(metrics_payload, f, indent=2)\n",
    "\n",
    "# Save factor for use at inference time (multiply future forecasts by this)\n",
    "with open(\"models/debias_factor.json\",\"w\") as f:\n",
    "    json.dump({\"debias_factor\": float(debias_factor)}, f)\n",
    "\n",
    "print(\"Saved:\")\n",
    "print(\"- reports/oof_predictions_debiased.csv\")\n",
    "print(\"- reports/cv_metrics_fixed.json\")\n",
    "print(\"- models/debias_factor.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd3871d0-011a-41ab-a158-7eecf17ac938",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'wape': 0.12252644036363322,\n",
       " 'smape': 0.4050997645856912,\n",
       " 'bias': 0.07364243695226826,\n",
       " 'rows': 13860}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- OOF-only metrics (rebuild the same validation windows) ---\n",
    "\n",
    "import os, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load OOF predictions saved earlier by your training cells\n",
    "work = pd.read_csv(\"reports/oof_predictions.csv\", parse_dates=[\"date\"])\n",
    "\n",
    "# If these aren't defined above, set them here\n",
    "HORIZON_DAYS   = 14\n",
    "MIN_TRAIN_DAYS = 150\n",
    "N_SPLITS       = 4\n",
    "\n",
    "def rolling_val_date_windows(dates, n_splits=N_SPLITS, min_train_days=MIN_TRAIN_DAYS, holdout_days=HORIZON_DAYS):\n",
    "    ud = np.array(sorted(pd.Series(dates).dt.normalize().unique()))\n",
    "    wins = []\n",
    "    for i in range(n_splits):\n",
    "        train_end = len(ud) - (n_splits - i) * holdout_days\n",
    "        if train_end < min_train_days:\n",
    "            continue\n",
    "        val_days = ud[train_end:train_end+holdout_days]\n",
    "        wins.append(pd.DatetimeIndex(val_days))\n",
    "    return wins\n",
    "\n",
    "val_windows = rolling_val_date_windows(work[\"date\"])\n",
    "\n",
    "# --- FIX: Flatten all DateTimeIndex windows (no union_many in pandas) ---\n",
    "if len(val_windows) == 0:\n",
    "    raise ValueError(\"No validation windows computed. Check dates or CV params.\")\n",
    "val_set = pd.DatetimeIndex(\n",
    "    np.unique(np.concatenate([w.values for w in val_windows]))\n",
    ")\n",
    "\n",
    "# Mark validation rows\n",
    "work[\"is_valid\"] = work[\"date\"].dt.normalize().isin(val_set)\n",
    "\n",
    "def WAPE(y, yhat, eps=1e-9):\n",
    "    y   = np.asarray(y); yhat = np.asarray(yhat)\n",
    "    return float(np.abs(y - yhat).sum() / (np.abs(y).sum() + eps))\n",
    "\n",
    "def SMAPE(y, yhat, eps=1e-9):\n",
    "    y   = np.asarray(y); yhat = np.asarray(yhat)\n",
    "    return float(np.mean(2*np.abs(y - yhat)/(np.abs(y)+np.abs(yhat)+eps)))\n",
    "\n",
    "def BIAS(y, yhat, eps=1e-9):\n",
    "    y   = np.asarray(y); yhat = np.asarray(yhat)\n",
    "    return float((yhat - y).sum() / (np.abs(y).sum() + eps))\n",
    "\n",
    "# Overall OOF metrics (validation rows only)\n",
    "m = work[\"is_valid\"]\n",
    "overall_oof = {\n",
    "    \"wape\": WAPE(work.loc[m, \"qty\"], work.loc[m, \"pred_oof\"]),\n",
    "    \"smape\": SMAPE(work.loc[m, \"qty\"], work.loc[m, \"pred_oof\"]),\n",
    "    \"bias\": BIAS(work.loc[m, \"qty\"], work.loc[m, \"pred_oof\"]),\n",
    "    \"rows\": int(m.sum())\n",
    "}\n",
    "overall_oof\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a9f1c43-7752-4a75-9d9b-81ecbb4f3150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOF (pre-debias): {'wape': 0.12252644036363322, 'smape': 0.4050997645856912, 'bias': 0.07364243695226826, 'rows': 13860}\n",
      "OOF (post-debias): {'wape': 0.10246324773588114, 'smape': 0.3945764006208121, 'bias': -1.2633210958279246e-16, 'rows': 13860, 'debias_factor_oof': 0.9314087871178826}\n",
      "Saved:\n",
      "- reports/oof_predictions_oofonly.csv\n",
      "- reports/cv_metrics_oofonly.json\n",
      "- models/debias_factor_oof.json\n"
     ]
    }
   ],
   "source": [
    "# --- Debias on OOF rows only, then save artifacts ---\n",
    "\n",
    "# Scale so total forecast == total actual on the OOF slice (bias ~ 0)\n",
    "num = float(work.loc[m, \"qty\"].sum())\n",
    "den = float(work.loc[m, \"pred_oof\"].sum()) + 1e-9\n",
    "debias_factor_oof = num / den\n",
    "\n",
    "work[\"pred_oof_debiased\"] = work[\"pred_oof\"] * debias_factor_oof\n",
    "\n",
    "overall_oof_post = {\n",
    "    \"wape\": WAPE(work.loc[m, \"qty\"], work.loc[m, \"pred_oof_debiased\"]),\n",
    "    \"smape\": SMAPE(work.loc[m, \"qty\"], work.loc[m, \"pred_oof_debiased\"]),\n",
    "    \"bias\": BIAS(work.loc[m, \"qty\"], work.loc[m, \"pred_oof_debiased\"]),\n",
    "    \"rows\": int(m.sum()),\n",
    "    \"debias_factor_oof\": debias_factor_oof\n",
    "}\n",
    "\n",
    "print(\"OOF (pre-debias):\", overall_oof)\n",
    "print(\"OOF (post-debias):\", overall_oof_post)\n",
    "\n",
    "# Save clean files\n",
    "os.makedirs(\"reports\", exist_ok=True)\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "work.to_csv(\"reports/oof_predictions_oofonly.csv\", index=False)\n",
    "with open(\"reports/cv_metrics_oofonly.json\",\"w\") as f:\n",
    "    json.dump({\"overall_oof_pre\": overall_oof, \"overall_oof_post\": overall_oof_post}, f, indent=2)\n",
    "with open(\"models/debias_factor_oof.json\",\"w\") as f:\n",
    "    json.dump({\"debias_factor_oof\": float(debias_factor_oof)}, f)\n",
    "\n",
    "print(\"Saved:\")\n",
    "print(\"- reports/oof_predictions_oofonly.csv\")\n",
    "print(\"- reports/cv_metrics_oofonly.json\")\n",
    "print(\"- models/debias_factor_oof.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fa9ab5a-a3a4-4d1b-8aa6-510ac2029ebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mnsuh\\AppData\\Local\\Temp\\ipykernel_103828\\1237077163.py:42: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: g.tail(28)[\"transactions\"].median())\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"None of ['store_id'] are in the columns\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_103828\\1237077163.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m     59\u001b[0m store_daily[\"store_roll7\"] = (store_daily.groupby(\"store_id\")[\"qty\"]\n\u001b[0;32m     60\u001b[0m                               \u001b[1;33m.\u001b[0m\u001b[0mrolling\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_periods\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m                               .reset_index(level=0, drop=True))\n\u001b[0;32m     62\u001b[0m last_store_r7 = (store_daily.groupby(\"store_id\")[\"store_roll7\"]\n\u001b[1;32m---> 63\u001b[1;33m                  .tail(1).reset_index().set_index(\"store_id\")[\"store_roll7\"])\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;31m# forecasting helpers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[0malpha\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m7.0\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# halflife=7\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive\\Desktop\\darkstore\\venv\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, keys, drop, append, inplace, verify_integrity)\u001b[0m\n\u001b[0;32m   6118\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mfound\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6119\u001b[0m                         \u001b[0mmissing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6121\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmissing\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6122\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"None of {missing} are in the columns\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6123\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6124\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6125\u001b[0m             \u001b[0mframe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"None of ['store_id'] are in the columns\""
     ]
    }
   ],
   "source": [
    "# === Forecast next 14 days (no dependency on `df`) ===\n",
    "# Reads history from reports/oof_predictions.csv and side tables from data/raw,\n",
    "# loads your saved two-stage models, applies debias_factor_oof, and writes forecasts.\n",
    "\n",
    "import os, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "H = 14\n",
    "RAW = \"data/raw\"\n",
    "REPORTS = \"reports\"\n",
    "MODELS = \"models\"\n",
    "\n",
    "# 1) Load models + feature list\n",
    "art = joblib.load(f\"{MODELS}/two_stage_lgbm.pkl\")\n",
    "clf, reg, FEATS = art[\"occ_model\"], art[\"size_model\"], art[\"features\"]\n",
    "\n",
    "# 2) Debias factor (OOF-only); fall back to 1.0 if missing\n",
    "debias_factor = 1.0\n",
    "try:\n",
    "    with open(f\"{MODELS}/debias_factor_oof.json\") as f:\n",
    "        debias_factor = float(json.load(f)[\"debias_factor_oof\"])\n",
    "except Exception:\n",
    "    print(\"NOTE: models/debias_factor_oof.json not found; using factor=1.0\")\n",
    "\n",
    "# 3) History (qty per storeÃ—skuÃ—day) from OOF file\n",
    "hist = pd.read_csv(f\"{REPORTS}/oof_predictions.csv\", parse_dates=[\"date\"])\n",
    "hist = hist.sort_values([\"store_id\",\"sku_id\",\"date\"]).reset_index(drop=True)\n",
    "\n",
    "last_hist_date = hist[\"date\"].max()\n",
    "future_dates = pd.date_range(last_hist_date + pd.Timedelta(days=1), periods=H, freq=\"D\")\n",
    "\n",
    "# 4) Side tables for future exogenous values\n",
    "txn = pd.read_csv(f\"{RAW}/transactions.csv\", parse_dates=[\"date\"]).rename(columns={\"store_nbr\":\"store_id\"})\n",
    "oil = pd.read_csv(f\"{RAW}/oil.csv\", parse_dates=[\"date\"]).rename(columns={\"dcoilwtico\":\"oil_price\"})\n",
    "hol = pd.read_csv(f\"{RAW}/holidays_events.csv\", parse_dates=[\"date\"])\n",
    "\n",
    "# store median transactions (last 28 days within history window)\n",
    "txn_recent = txn[txn[\"date\"] <= last_hist_date].sort_values([\"store_id\",\"date\"])\n",
    "txn_med = (txn_recent.groupby(\"store_id\")\n",
    "                     .apply(lambda g: g.tail(28)[\"transactions\"].median())\n",
    "                     .rename(\"txn_med\"))\n",
    "\n",
    "# last oil price up to history end\n",
    "oil = oil.sort_values(\"date\")\n",
    "oil[\"oil_price\"] = oil[\"oil_price\"].ffill().bfill()\n",
    "last_oil = float(oil[oil[\"date\"] <= last_hist_date][\"oil_price\"].iloc[-1])\n",
    "\n",
    "# simple holiday set (national + additional, not transferred)\n",
    "hol_flag = (hol[hol[\"type\"].isin([\"Holiday\",\"Additional\"])]\n",
    "              .loc[~hol[\"transferred\"].fillna(False), [\"date\"]]\n",
    "              .drop_duplicates())\n",
    "holiday_set = set(hol_flag[\"date\"].dt.normalize().tolist())\n",
    "\n",
    "# store-level rolling-7 of total qty (last value per store)\n",
    "store_daily = (hist.groupby([\"store_id\",\"date\"])[\"qty\"].sum().reset_index())\n",
    "store_daily = store_daily.sort_values([\"store_id\",\"date\"])\n",
    "store_daily[\"store_roll7\"] = (store_daily.groupby(\"store_id\")[\"qty\"]\n",
    "                              .rolling(7, min_periods=1).mean()\n",
    "                              .reset_index(level=0, drop=True))\n",
    "last_store_r7 = (store_daily.groupby(\"store_id\")[\"store_roll7\"]\n",
    "                 .tail(1).reset_index().set_index(\"store_id\")[\"store_roll7\"])\n",
    "\n",
    "# forecasting helpers\n",
    "alpha = 1 - np.exp(-np.log(2)/7.0)  # halflife=7\n",
    "\n",
    "def forecast_pair(g):\n",
    "    g = g.sort_values(\"date\").copy()\n",
    "    store_id = g[\"store_id\"].iloc[0]\n",
    "    sku_id   = g[\"sku_id\"].iloc[0]\n",
    "\n",
    "    # last 60 days for lags/rolls\n",
    "    qty = g[\"qty\"].astype(float).tolist()[-60:]\n",
    "    ewm = 0.0\n",
    "    for v in qty:\n",
    "        ewm = alpha * v + (1 - alpha) * ewm\n",
    "\n",
    "    store_r7 = float(last_store_r7.get(store_id, np.mean(qty[-7:]) if qty else 0.0))\n",
    "    tx_med   = float(txn_med.get(store_id, 0.0))\n",
    "\n",
    "    rows = []\n",
    "    for d in future_dates:\n",
    "        def lag(k): return qty[-k] if len(qty) >= k else 0.0\n",
    "        last7, last28 = (qty[-7:] if len(qty)>=7 else qty[:], qty[-28:] if len(qty)>=28 else qty[:])\n",
    "        roll7_mean, roll7_std  = (float(np.mean(last7)) if last7 else 0.0, float(np.std(last7)) if last7 else 0.0)\n",
    "        roll28_mean = float(np.mean(last28)) if last28 else 0.0\n",
    "\n",
    "        dow, month = d.weekday(), d.month\n",
    "        is_weekend = int(dow >= 5)\n",
    "        is_month_end = int((d + pd.Timedelta(days=1)).month != month)\n",
    "\n",
    "        feat_row = {\n",
    "            \"dow\": dow, \"month\": month, \"is_weekend\": is_weekend, \"is_month_end\": is_month_end,\n",
    "            \"lag_1\": lag(1), \"lag_7\": lag(7), \"lag_14\": lag(14),\n",
    "            \"roll7_mean\": roll7_mean, \"roll7_std\": roll7_std,\n",
    "            \"roll28_mean\": roll28_mean, \"exp_decay_7\": ewm,\n",
    "            \"store_roll7_mean\": store_r7,\n",
    "            \"is_promo\": 0,  # change if you have promo plans\n",
    "            \"transactions\": tx_med,\n",
    "            \"oil_price\": last_oil,\n",
    "            \"holiday_flag\": 1 if d.normalize() in holiday_set else 0\n",
    "        }\n",
    "\n",
    "        x = [feat_row.get(f, 0.0) for f in FEATS]\n",
    "        p_occ = float(clf.predict_proba([x])[0, 1])\n",
    "        size  = float(max(0.0, reg.predict([x])[0]))\n",
    "        yhat  = p_occ * size\n",
    "\n",
    "        rows.append({\"store_id\":store_id, \"sku_id\":sku_id, \"date\":d,\n",
    "                     \"p_occ\":p_occ, \"size_pred\":size, \"forecast_raw\":yhat})\n",
    "\n",
    "        # simulate forward for next day's lags/rolls\n",
    "        qty.append(yhat)\n",
    "        ewm = alpha * yhat + (1 - alpha) * ewm\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# Run for every pair\n",
    "pairs = hist[[\"store_id\",\"sku_id\"]].drop_duplicates()\n",
    "fcasts = []\n",
    "for _, row in pairs.iterrows():\n",
    "    g = hist[(hist[\"store_id\"]==row[\"store_id\"]) & (hist[\"sku_id\"]==row[\"sku_id\"])]\n",
    "    if len(g):\n",
    "        fcasts.append(forecast_pair(g))\n",
    "\n",
    "future = pd.concat(fcasts, ignore_index=True) if len(fcasts) else pd.DataFrame(columns=[\"store_id\",\"sku_id\",\"date\",\"forecast_raw\"])\n",
    "future[\"forecast\"] = future[\"forecast_raw\"] * debias_factor\n",
    "\n",
    "# Save\n",
    "os.makedirs(REPORTS, exist_ok=True)\n",
    "out_path = f\"{REPORTS}/forecast_next_14d.csv\"\n",
    "future.to_csv(out_path, index=False)\n",
    "print(f\"Saved forecasts to {out_path}  |  rows: {len(future)}\")\n",
    "future.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f88a8f9d-9f41-459d-adea-b4a826c5c507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved forecasts to reports/forecast_next_14d.csv  |  rows: 4620\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>store_id</th>\n",
       "      <th>sku_id</th>\n",
       "      <th>date</th>\n",
       "      <th>p_occ</th>\n",
       "      <th>size_pred</th>\n",
       "      <th>forecast_raw</th>\n",
       "      <th>forecast</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>AUTOMOTIVE</td>\n",
       "      <td>2017-08-16</td>\n",
       "      <td>0.999990</td>\n",
       "      <td>10.243966</td>\n",
       "      <td>10.243865</td>\n",
       "      <td>9.541226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>AUTOMOTIVE</td>\n",
       "      <td>2017-08-17</td>\n",
       "      <td>0.999993</td>\n",
       "      <td>9.166079</td>\n",
       "      <td>9.166018</td>\n",
       "      <td>8.537310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>AUTOMOTIVE</td>\n",
       "      <td>2017-08-18</td>\n",
       "      <td>0.999996</td>\n",
       "      <td>11.224546</td>\n",
       "      <td>11.224503</td>\n",
       "      <td>10.454600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>AUTOMOTIVE</td>\n",
       "      <td>2017-08-19</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>14.316113</td>\n",
       "      <td>14.316075</td>\n",
       "      <td>13.334118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>AUTOMOTIVE</td>\n",
       "      <td>2017-08-20</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>17.286934</td>\n",
       "      <td>17.286917</td>\n",
       "      <td>16.101187</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   store_id      sku_id       date     p_occ  size_pred  forecast_raw  \\\n",
       "0         3  AUTOMOTIVE 2017-08-16  0.999990  10.243966     10.243865   \n",
       "1         3  AUTOMOTIVE 2017-08-17  0.999993   9.166079      9.166018   \n",
       "2         3  AUTOMOTIVE 2017-08-18  0.999996  11.224546     11.224503   \n",
       "3         3  AUTOMOTIVE 2017-08-19  0.999997  14.316113     14.316075   \n",
       "4         3  AUTOMOTIVE 2017-08-20  0.999999  17.286934     17.286917   \n",
       "\n",
       "    forecast  \n",
       "0   9.541226  \n",
       "1   8.537310  \n",
       "2  10.454600  \n",
       "3  13.334118  \n",
       "4  16.101187  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Forecast next 14 days (fixed) ===\n",
    "# Reads history from reports/oof_predictions.csv & raw side tables,\n",
    "# loads saved two-stage models, applies OOF debias factor, and writes forecasts.\n",
    "\n",
    "import os, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "H = 14\n",
    "RAW = \"data/raw\"\n",
    "REPORTS = \"reports\"\n",
    "MODELS = \"models\"\n",
    "\n",
    "# 1) Load models + feature list\n",
    "art = joblib.load(f\"{MODELS}/two_stage_lgbm.pkl\")\n",
    "clf, reg, FEATS = art[\"occ_model\"], art[\"size_model\"], art[\"features\"]\n",
    "\n",
    "# 2) Debias factor (OOF-only); fall back to 1.0 if missing\n",
    "debias_factor = 1.0\n",
    "try:\n",
    "    with open(f\"{MODELS}/debias_factor_oof.json\") as f:\n",
    "        debias_factor = float(json.load(f)[\"debias_factor_oof\"])\n",
    "except Exception:\n",
    "    print(\"NOTE: models/debias_factor_oof.json not found; using factor=1.0\")\n",
    "\n",
    "# 3) History (qty per storeÃ—skuÃ—day)\n",
    "hist = pd.read_csv(f\"{REPORTS}/oof_predictions.csv\", parse_dates=[\"date\"])\n",
    "hist = hist.sort_values([\"store_id\",\"sku_id\",\"date\"]).reset_index(drop=True)\n",
    "\n",
    "last_hist_date = hist[\"date\"].max()\n",
    "future_dates = pd.date_range(last_hist_date + pd.Timedelta(days=1), periods=H, freq=\"D\")\n",
    "\n",
    "# 4) Side tables for future exogenous values\n",
    "txn = pd.read_csv(f\"{RAW}/transactions.csv\", parse_dates=[\"date\"]).rename(columns={\"store_nbr\":\"store_id\"})\n",
    "oil = pd.read_csv(f\"{RAW}/oil.csv\", parse_dates=[\"date\"]).rename(columns={\"dcoilwtico\":\"oil_price\"})\n",
    "hol = pd.read_csv(f\"{RAW}/holidays_events.csv\", parse_dates=[\"date\"])\n",
    "\n",
    "# -- FIX 1: transactions median (no deprecation; last 28 days per store)\n",
    "txn_recent = txn.loc[txn[\"date\"] <= last_hist_date].sort_values([\"store_id\",\"date\"]).copy()\n",
    "txn_med = (\n",
    "    txn_recent.groupby(\"store_id\", group_keys=False)[\"transactions\"]\n",
    "    .apply(lambda s: s.tail(28).median())\n",
    "    .rename(\"txn_med\")\n",
    ")\n",
    "\n",
    "# Oil price: carry forward the last known value\n",
    "oil = oil.sort_values(\"date\")\n",
    "oil[\"oil_price\"] = oil[\"oil_price\"].ffill().bfill()\n",
    "last_oil = float(oil[oil[\"date\"] <= last_hist_date][\"oil_price\"].iloc[-1])\n",
    "\n",
    "# Simple holiday set (national + additional, not transferred)\n",
    "hol_flag = (\n",
    "    hol[hol[\"type\"].isin([\"Holiday\",\"Additional\"])]\n",
    "    .loc[~hol[\"transferred\"].fillna(False), [\"date\"]]\n",
    "    .drop_duplicates()\n",
    ")\n",
    "holiday_set = set(hol_flag[\"date\"].dt.normalize().tolist())\n",
    "\n",
    "# Store-level rolling-7 of total qty (last per store)\n",
    "store_daily = (\n",
    "    hist.groupby([\"store_id\",\"date\"], as_index=False)[\"qty\"].sum()\n",
    "    .sort_values([\"store_id\",\"date\"])\n",
    ")\n",
    "store_daily[\"store_roll7\"] = (\n",
    "    store_daily.groupby(\"store_id\")[\"qty\"]\n",
    "    .rolling(7, min_periods=1).mean()\n",
    "    .reset_index(level=0, drop=True)\n",
    ")\n",
    "# -- FIX 2: get last value per store safely (no missing 'store_id' column)\n",
    "last_store_r7 = (\n",
    "    store_daily.groupby(\"store_id\", as_index=True)[\"store_roll7\"]\n",
    "    .last()\n",
    ").fillna(0.0)\n",
    "\n",
    "# forecasting helpers\n",
    "alpha = 1 - np.exp(-np.log(2)/7.0)  # halflife=7\n",
    "\n",
    "def forecast_pair(g):\n",
    "    g = g.sort_values(\"date\").copy()\n",
    "    store_id = g[\"store_id\"].iloc[0]\n",
    "    sku_id   = g[\"sku_id\"].iloc[0]\n",
    "\n",
    "    # last 60 days for lags/rolls\n",
    "    qty = g[\"qty\"].astype(float).tolist()[-60:]\n",
    "    ewm = 0.0\n",
    "    for v in qty:\n",
    "        ewm = alpha * v + (1 - alpha) * ewm\n",
    "\n",
    "    store_r7 = float(last_store_r7.get(store_id, np.mean(qty[-7:]) if qty else 0.0))\n",
    "    tx_med   = float(txn_med.get(store_id, 0.0))\n",
    "\n",
    "    rows = []\n",
    "    for d in future_dates:\n",
    "        def lag(k): return qty[-k] if len(qty) >= k else 0.0\n",
    "        last7  = qty[-7:]  if len(qty) >= 7  else qty[:]\n",
    "        last28 = qty[-28:] if len(qty) >= 28 else qty[:]\n",
    "        roll7_mean  = float(np.mean(last7))  if last7 else 0.0\n",
    "        roll7_std   = float(np.std(last7))   if last7 else 0.0\n",
    "        roll28_mean = float(np.mean(last28)) if last28 else 0.0\n",
    "\n",
    "        dow, month = d.weekday(), d.month\n",
    "        is_weekend   = int(dow >= 5)\n",
    "        is_month_end = int((d + pd.Timedelta(days=1)).month != month)\n",
    "\n",
    "        feat_row = {\n",
    "            \"dow\": dow, \"month\": month, \"is_weekend\": is_weekend, \"is_month_end\": is_month_end,\n",
    "            \"lag_1\": lag(1), \"lag_7\": lag(7), \"lag_14\": lag(14),\n",
    "            \"roll7_mean\": roll7_mean, \"roll7_std\": roll7_std,\n",
    "            \"roll28_mean\": roll28_mean, \"exp_decay_7\": ewm,\n",
    "            \"store_roll7_mean\": store_r7,\n",
    "            \"is_promo\": 0,                      # adjust if you have promo plans\n",
    "            \"transactions\": tx_med,\n",
    "            \"oil_price\": last_oil,\n",
    "            \"holiday_flag\": 1 if d.normalize() in holiday_set else 0\n",
    "        }\n",
    "\n",
    "        x = [feat_row.get(f, 0.0) for f in FEATS]\n",
    "        p_occ = float(clf.predict_proba([x])[0, 1])\n",
    "        size  = float(max(0.0, reg.predict([x])[0]))\n",
    "        yhat  = p_occ * size\n",
    "\n",
    "        rows.append({\n",
    "            \"store_id\": store_id, \"sku_id\": sku_id, \"date\": d,\n",
    "            \"p_occ\": p_occ, \"size_pred\": size, \"forecast_raw\": yhat\n",
    "        })\n",
    "\n",
    "        # simulate forward for next day's lags/rolls\n",
    "        qty.append(yhat)\n",
    "        ewm = alpha * yhat + (1 - alpha) * ewm\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# Run for every pair\n",
    "pairs = hist[[\"store_id\",\"sku_id\"]].drop_duplicates()\n",
    "fcasts = []\n",
    "for _, row in pairs.iterrows():\n",
    "    g = hist[(hist[\"store_id\"]==row[\"store_id\"]) & (hist[\"sku_id\"]==row[\"sku_id\"])]\n",
    "    if len(g):\n",
    "        fcasts.append(forecast_pair(g))\n",
    "\n",
    "future = pd.concat(fcasts, ignore_index=True) if len(fcasts) else pd.DataFrame(columns=[\"store_id\",\"sku_id\",\"date\",\"forecast_raw\"])\n",
    "future[\"forecast\"] = future[\"forecast_raw\"] * debias_factor\n",
    "\n",
    "# Save\n",
    "os.makedirs(REPORTS, exist_ok=True)\n",
    "out_path = f\"{REPORTS}/forecast_next_14d.csv\"\n",
    "future.to_csv(out_path, index=False)\n",
    "print(f\"Saved forecasts to {out_path}  |  rows: {len(future)}\")\n",
    "future.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "042a48f4-1d55-4892-90dd-6e9404e7908b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next 14d total forecast: 3729726.1642138986\n",
      "Last 14d total actual  : 4162290.0694049997\n",
      "\n",
      "Top stores by 14d forecast:\n",
      " store_id\n",
      "44    538534.501547\n",
      "45    491445.931412\n",
      "47    449342.982488\n",
      "3     419872.673262\n",
      "49    410605.604819\n",
      "46    343242.301690\n",
      "48    289798.655967\n",
      "51    286002.146433\n",
      "50    253849.431103\n",
      "8     247031.935493\n",
      "Name: forecast, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "oof = pd.read_csv(\"reports/oof_predictions_oofonly.csv\", parse_dates=[\"date\"])\n",
    "fut = pd.read_csv(\"reports/forecast_next_14d.csv\", parse_dates=[\"date\"])\n",
    "\n",
    "print(\"Next 14d total forecast:\", fut[\"forecast\"].sum())\n",
    "last14 = oof.sort_values(\"date\").groupby([\"store_id\",\"sku_id\"]).tail(14)\n",
    "print(\"Last 14d total actual  :\", last14[\"qty\"].sum())\n",
    "\n",
    "by_store = fut.groupby(\"store_id\")[\"forecast\"].sum().sort_values(ascending=False).head(10)\n",
    "print(\"\\nTop stores by 14d forecast:\\n\", by_store)\n",
    "\n",
    "def plot_pair(store_id, sku_id):\n",
    "    hist = oof[(oof.store_id==store_id) & (oof.sku_id==sku_id)].sort_values(\"date\").tail(60)\n",
    "    nxt  = fut[(fut.store_id==store_id) & (fut.sku_id==sku_id)].sort_values(\"date\")\n",
    "    plt.figure(figsize=(9,3))\n",
    "    plt.plot(hist[\"date\"], hist[\"qty\"], label=\"actual (last 60d)\")\n",
    "    plt.plot(hist[\"date\"], hist.get(\"pred_oof_debiased\", hist.get(\"pred_oof\")), label=\"pred (OOF)\", alpha=0.7)\n",
    "    plt.plot(nxt[\"date\"], nxt[\"forecast\"], label=\"forecast (next 14d)\")\n",
    "    plt.legend(); plt.title(f\"Store {store_id} | SKU {sku_id}\"); plt.show()\n",
    "\n",
    "# example (replace with a real pair from your data)\n",
    "# plot_pair(store_id=1, sku_id=\"BEVERAGES\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebf3d720-a24c-4b51-b6ef-11582fb9f2f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>store_id</th>\n",
       "      <th>sku_id</th>\n",
       "      <th>sigma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>AUTOMOTIVE</td>\n",
       "      <td>4.472280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>BABY CARE</td>\n",
       "      <td>1.601356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>BEAUTY</td>\n",
       "      <td>4.812473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>BEVERAGES</td>\n",
       "      <td>1062.271649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>BOOKS</td>\n",
       "      <td>0.007447</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   store_id      sku_id        sigma\n",
       "0         3  AUTOMOTIVE     4.472280\n",
       "1         3   BABY CARE     1.601356\n",
       "2         3      BEAUTY     4.812473\n",
       "3         3   BEVERAGES  1062.271649\n",
       "4         3       BOOKS     0.007447"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np, pandas as pd\n",
    "\n",
    "# Use debiased OOF if available; else use pred_oof\n",
    "oof = pd.read_csv(\"reports/oof_predictions_oofonly.csv\", parse_dates=[\"date\"])\n",
    "pred_col = \"pred_oof_debiased\" if \"pred_oof_debiased\" in oof.columns else \"pred_oof\"\n",
    "oof = oof[oof[\"is_valid\"]] if \"is_valid\" in oof.columns else oof  # ensure only val rows\n",
    "\n",
    "oof[\"resid\"] = oof[\"qty\"] - oof[pred_col]\n",
    "\n",
    "# per-pair residual std; guard for tiny samples\n",
    "sigma_pair = (oof.groupby([\"store_id\",\"sku_id\"])[\"resid\"]\n",
    "                .agg(lambda s: float(np.std(s)) if len(s)>=7 else np.nan)\n",
    "                .rename(\"sigma\"))\n",
    "\n",
    "# fallbacks: by store, then global\n",
    "sigma_store = (oof.groupby(\"store_id\")[\"resid\"].std().rename(\"sigma_store\"))\n",
    "sigma_global = float(oof[\"resid\"].std())\n",
    "\n",
    "sig = sigma_pair.reset_index()\n",
    "sig = sig.merge(sigma_store.reset_index(), on=\"store_id\", how=\"left\")\n",
    "sig[\"sigma_filled\"] = sig[\"sigma\"].fillna(sig[\"sigma_store\"]).fillna(sigma_global)\n",
    "sig = sig[[\"store_id\",\"sku_id\",\"sigma_filled\"]].rename(columns={\"sigma_filled\":\"sigma\"})\n",
    "sig.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1fd5aa63-469a-4ab4-8022-6387649c2065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: reports/order_plan_L2d.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>store_id</th>\n",
       "      <th>sku_id</th>\n",
       "      <th>abc_class</th>\n",
       "      <th>need_L</th>\n",
       "      <th>safety_stock</th>\n",
       "      <th>order_qty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>BEVERAGES</td>\n",
       "      <td>B</td>\n",
       "      <td>12912.109300</td>\n",
       "      <td>1925.921644</td>\n",
       "      <td>14838.030944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>3</td>\n",
       "      <td>PRODUCE</td>\n",
       "      <td>B</td>\n",
       "      <td>13577.043730</td>\n",
       "      <td>1168.525354</td>\n",
       "      <td>14745.569084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3</td>\n",
       "      <td>GROCERY I</td>\n",
       "      <td>B</td>\n",
       "      <td>12561.338603</td>\n",
       "      <td>1257.841116</td>\n",
       "      <td>13819.179719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>DAIRY</td>\n",
       "      <td>C</td>\n",
       "      <td>4188.820095</td>\n",
       "      <td>270.295587</td>\n",
       "      <td>4459.115681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>CLEANING</td>\n",
       "      <td>C</td>\n",
       "      <td>3549.418772</td>\n",
       "      <td>382.100357</td>\n",
       "      <td>3931.519128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>BREAD/BAKERY</td>\n",
       "      <td>C</td>\n",
       "      <td>2245.404625</td>\n",
       "      <td>146.834981</td>\n",
       "      <td>2392.239606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>3</td>\n",
       "      <td>POULTRY</td>\n",
       "      <td>C</td>\n",
       "      <td>2157.491713</td>\n",
       "      <td>146.520202</td>\n",
       "      <td>2304.011915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>3</td>\n",
       "      <td>MEATS</td>\n",
       "      <td>C</td>\n",
       "      <td>1111.722762</td>\n",
       "      <td>112.154403</td>\n",
       "      <td>1223.877165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>3</td>\n",
       "      <td>PERSONAL CARE</td>\n",
       "      <td>C</td>\n",
       "      <td>988.152608</td>\n",
       "      <td>101.758965</td>\n",
       "      <td>1089.911573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3</td>\n",
       "      <td>HOME CARE</td>\n",
       "      <td>C</td>\n",
       "      <td>957.799902</td>\n",
       "      <td>118.480678</td>\n",
       "      <td>1076.280580</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    store_id         sku_id abc_class        need_L  safety_stock  \\\n",
       "3          3      BEVERAGES         B  12912.109300   1925.921644   \n",
       "30         3        PRODUCE         B  13577.043730   1168.525354   \n",
       "12         3      GROCERY I         B  12561.338603   1257.841116   \n",
       "8          3          DAIRY         C   4188.820095    270.295587   \n",
       "7          3       CLEANING         C   3549.418772    382.100357   \n",
       "5          3   BREAD/BAKERY         C   2245.404625    146.834981   \n",
       "28         3        POULTRY         C   2157.491713    146.520202   \n",
       "24         3          MEATS         C   1111.722762    112.154403   \n",
       "25         3  PERSONAL CARE         C    988.152608    101.758965   \n",
       "18         3      HOME CARE         C    957.799902    118.480678   \n",
       "\n",
       "       order_qty  \n",
       "3   14838.030944  \n",
       "30  14745.569084  \n",
       "12  13819.179719  \n",
       "8    4459.115681  \n",
       "7    3931.519128  \n",
       "5    2392.239606  \n",
       "28   2304.011915  \n",
       "24   1223.877165  \n",
       "25   1089.911573  \n",
       "18   1076.280580  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np, pandas as pd\n",
    "\n",
    "# inputs you can change\n",
    "LEAD_TIME_DAYS = 2\n",
    "service_z = {\"A\": 1.645, \"B\": 1.282, \"C\": 0.842}  # 95/90/80%\n",
    "\n",
    "# ABC classes (rebuild quickly from OOF totals)\n",
    "oof = pd.read_csv(\"reports/oof_predictions_oofonly.csv\", parse_dates=[\"date\"])\n",
    "pair_vol = oof.groupby([\"store_id\",\"sku_id\"])[\"qty\"].sum().sort_values(ascending=False)\n",
    "cum = (pair_vol.cumsum()/pair_vol.sum())\n",
    "def _cls(x): return \"A\" if x<=0.2 else (\"B\" if x<=0.5 else \"C\")\n",
    "abc = cum.map(_cls).rename(\"abc_class\").reset_index()\n",
    "\n",
    "# future sum over lead time\n",
    "fut = pd.read_csv(\"reports/forecast_next_14d.csv\", parse_dates=[\"date\"])\n",
    "need = (fut.sort_values(\"date\")\n",
    "          .groupby([\"store_id\",\"sku_id\"])[\"forecast\"]\n",
    "          .apply(lambda s: float(s.head(LEAD_TIME_DAYS).sum()))\n",
    "          .rename(\"need_L\")\n",
    "          .reset_index())\n",
    "\n",
    "# join sigma and ABC\n",
    "plan = (need.merge(sig, on=[\"store_id\",\"sku_id\"], how=\"left\")\n",
    "             .merge(abc, on=[\"store_id\",\"sku_id\"], how=\"left\"))\n",
    "\n",
    "# safety stock = z * sigma * sqrt(L)  (absolute units)\n",
    "plan[\"z\"] = plan[\"abc_class\"].map(service_z).fillna(service_z[\"C\"])\n",
    "plan[\"sigma\"] = plan[\"sigma\"].fillna(plan[\"sigma\"].median())  # last resort fallback\n",
    "plan[\"safety_stock\"] = plan[\"z\"] * plan[\"sigma\"] * np.sqrt(LEAD_TIME_DAYS)\n",
    "\n",
    "# on-hand/on-order (Favorita has none â†’ 0)\n",
    "plan[\"on_hand\"] = 0.0\n",
    "plan[\"on_order\"] = 0.0\n",
    "\n",
    "# final order qty\n",
    "plan[\"order_qty\"] = np.maximum(0.0, plan[\"need_L\"] + plan[\"safety_stock\"] - plan[\"on_hand\"] - plan[\"on_order\"])\n",
    "\n",
    "# tidy + save\n",
    "cols = [\"store_id\",\"sku_id\",\"abc_class\",\"need_L\",\"safety_stock\",\"order_qty\"]\n",
    "order_plan = plan[cols].sort_values([\"store_id\",\"order_qty\"], ascending=[True, False])\n",
    "order_plan.to_csv(\"reports/order_plan_L{}d.csv\".format(LEAD_TIME_DAYS), index=False)\n",
    "print(\"Saved:\", f\"reports/order_plan_L{LEAD_TIME_DAYS}d.csv\")\n",
    "order_plan.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d077706d-132e-4e27-aeb0-1796c8405e92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
